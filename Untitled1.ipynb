{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82c09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "#nltk.download('stopwords')\n",
    "import pickle\n",
    "#from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360cb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode(df, lst_cols, fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    if (lst_cols is not None\n",
    "        and len(lst_cols) > 0\n",
    "        and not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997862d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "my_data=load_files(r'C:\\ML Training Docs\\New folder\\split\\Train')\n",
    "X,y = my_data.data, my_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5ec9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\ML Training Docs\\New folder\\split\\Train\\MDU...</td>\n",
       "      <td>Solicitor Contacts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\ML Training Docs\\New folder\\split\\Train\\GMC...</td>\n",
       "      <td>GMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\ML Training Docs\\New folder\\split\\Train\\MDU...</td>\n",
       "      <td>MDU Invoices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\ML Training Docs\\New folder\\split\\Train\\MDU...</td>\n",
       "      <td>Solicitor Contacts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\ML Training Docs\\New folder\\split\\Train\\3rd...</td>\n",
       "      <td>3rd Party Expert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           filenames              target\n",
       "0  C:\\ML Training Docs\\New folder\\split\\Train\\MDU...  Solicitor Contacts\n",
       "0  C:\\ML Training Docs\\New folder\\split\\Train\\GMC...                 GMC\n",
       "0  C:\\ML Training Docs\\New folder\\split\\Train\\MDU...        MDU Invoices\n",
       "0  C:\\ML Training Docs\\New folder\\split\\Train\\MDU...  Solicitor Contacts\n",
       "0  C:\\ML Training Docs\\New folder\\split\\Train\\3rd...    3rd Party Expert"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.DataFrame({'filenames':[my_data.filenames],'target':[my_data.target]})\n",
    "df=explode(df,['filenames','target'],fill_value='',preserve_index=True)\n",
    "df['target']=df['target'].replace([0,1,2,3,4],['3rd Party Expert','GMC','Solicitor Contacts','MDU Invoices','NHSLA'])\n",
    "# df.to_csv(r'C:\\ML Training Docs\\New folder\\data_final\\data_final.csv',index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a24c057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zz2839\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.set_proxy('http://Boopathi.Murugesan:Welcometothemdu2021@proxy.mdu.local:8080')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\\\n', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\\\t', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\\\', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r' +', ' ', document)\n",
    "    \n",
    "#     # Removing prefixed 'b'\n",
    "#     document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(e for e in document if e.isalnum())\n",
    "    document = ' '.join([w for w in document.split() if len(w)>1])\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7513e932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1021 1164 12 february 2014 your by email strictly private confidential dr james garritt the priory belmont road douglas isle of man im1 4ns dear james mdu dr james garritt gdc investigation am finalising the draft letter of observation for the gdc and am writing to obtain some further in the of the computer record for which received earlier this contain entry of bpe score recorded on 19 april 23 may 2013 and october none of the previous computer of record for ke contain these bpe this raise potentially serious concern and having discussed the entry with bryan he would like me to arrange for to have three way telephone conference tomorrow morning at some time between and should be grateful if you would let me know when would be convenient for you and the best number to contact you on 26 april phil lucas made handwritten note in respect of this hygiene appointment but did not make any entry in the computer should be grateful if we could discus this also during tomorrow xe2 x80 x99s you kindly provided the upper removable appliance referred to in the clinical record for so on 18 april you also mentioned in recent email that you had located the peer review note in respect of the if these note have been should be grateful to receive copy but in any please confirm the when you took over responsibility for treating so on 22 november you undertook an orthodontic assessment and obtained signed consent from can you please provide me with copy of the prescription submitted to invisalign in respect of the ongoing treatment involving refinement can you confirm if you provided written treatment plan to so on or around 22 november 2013 and if may have copy wondered if you have had an opportunity to book course in relation to note radiography and diagnosis and treatment of periodontal condition previously if should be grateful to receive copy of the booking confirmation for inclusion in the letter of regard cpd more note that all of the cpd undertaken in your current cpd cycle believe commenced on january relates to orthodontic and cosmetic bryan harvey ha asked that draw to your attention the need to undertake core cpd course also and enclose copy of the gdc xe2 x80 x99s cpd booklet which may be of the draft observation for the ic are running to around 18 much of which relates to responding to the allegation which there are around 90 in order to ensure that you have sufficient time to consider the observation have requested an extension of time of one to friday 19 february 2014 for providing the finalised observation to the enclose copy of my letter to the gdc for your consider it likely that the gdc will grant an extension of few day but perhaps not full look forward to hearing from you in relation to arranging call tomorrow with kind yours sincerely niamh rigby associate solicitor 020 7484 7514 020 7839 8212 enc']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1202671",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = documents\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "clean_document = [' '.join([b for b in a.split() if b not in stop_words]) for a in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b500ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1021 1164 12 february 2014 email strictly private confidential dr james garritt priory belmont road douglas isle man im1 4ns dear james mdu dr james garritt gdc investigation finalising draft letter observation gdc writing obtain computer record received earlier contain entry bpe score recorded 19 april 23 may 2013 october none previous computer record ke contain bpe raise potentially serious concern discussed entry bryan would like arrange three way telephone conference tomorrow morning time grateful would let know would convenient best number contact 26 april phil lucas made handwritten note respect hygiene appointment make entry computer grateful could discus also tomorrow xe2 x80 x99s kindly provided upper removable appliance referred clinical record 18 april also mentioned recent email located peer review note respect note grateful receive copy please confirm took responsibility treating 22 november undertook orthodontic assessment obtained signed consent please provide copy prescription submitted invisalign respect ongoing treatment involving refinement confirm provided written treatment plan around 22 november 2013 may copy wondered opportunity book course relation note radiography diagnosis treatment periodontal condition previously grateful receive copy booking confirmation inclusion letter regard cpd note cpd undertaken current cpd cycle believe commenced january relates orthodontic cosmetic bryan harvey ha asked draw attention need undertake core cpd course also enclose copy gdc xe2 x80 x99s cpd booklet may draft observation ic running around 18 much relates responding allegation around 90 order ensure sufficient time consider observation requested extension time one friday 19 february 2014 providing finalised observation enclose copy letter gdc consider likely gdc grant extension day perhaps full look forward hearing relation arranging call tomorrow kind sincerely niamh rigby associate solicitor 020 7484 7514 020 7839 8212 enc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_document[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af8a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7)\n",
    "X = vectorizer.fit_transform(clean_document).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb07943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b76bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d540b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest: 0.8651960784313726\n",
      "[[75  2  0  0  8]\n",
      " [ 7 66  0  0  3]\n",
      " [ 0  3 58  2 14]\n",
      " [ 0  0  2 85  0]\n",
      " [ 5  1  8  0 69]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87        85\n",
      "           1       0.92      0.87      0.89        76\n",
      "           2       0.85      0.75      0.80        77\n",
      "           3       0.98      0.98      0.98        87\n",
      "           4       0.73      0.83      0.78        83\n",
      "\n",
      "    accuracy                           0.87       408\n",
      "   macro avg       0.87      0.86      0.86       408\n",
      "weighted avg       0.87      0.87      0.87       408\n",
      "\n",
      "Accuracy score (training): 0.995\n",
      "Accuracy score (test): 0.865\n"
     ]
    }
   ],
   "source": [
    "'''training the data in RF model'''\n",
    "from sklearn.ensemble._forest import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "classifier=RandomForestClassifier()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "print('RandomForest:',accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(classifier.score(X_train, y_train)))\n",
    "print(\"Accuracy score (test): {0:.3f}\".format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feb9985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier: 0.8725490196078431\n",
      "[[79  3  0  0  3]\n",
      " [ 5 66  2  0  3]\n",
      " [ 0  4 60  3 10]\n",
      " [ 0  0  5 82  0]\n",
      " [ 4  2  7  1 69]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91        85\n",
      "           1       0.88      0.87      0.87        76\n",
      "           2       0.81      0.78      0.79        77\n",
      "           3       0.95      0.94      0.95        87\n",
      "           4       0.81      0.83      0.82        83\n",
      "\n",
      "    accuracy                           0.87       408\n",
      "   macro avg       0.87      0.87      0.87       408\n",
      "weighted avg       0.87      0.87      0.87       408\n",
      "\n",
      "Accuracy score (training): 0.994\n",
      "Accuracy score (test): 0.873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier=GradientBoostingClassifier()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "print('GradientBoostingClassifier:',accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(classifier.score(X_train, y_train)))\n",
    "print(\"Accuracy score (test): {0:.3f}\".format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52f7443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier: 0.8725490196078431\n",
      "[[78  4  0  0  3]\n",
      " [ 3 68  2  0  3]\n",
      " [ 1  4 60  3  9]\n",
      " [ 0  0  5 82  0]\n",
      " [ 4  2  8  1 68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91        85\n",
      "           1       0.87      0.89      0.88        76\n",
      "           2       0.80      0.78      0.79        77\n",
      "           3       0.95      0.94      0.95        87\n",
      "           4       0.82      0.82      0.82        83\n",
      "\n",
      "    accuracy                           0.87       408\n",
      "   macro avg       0.87      0.87      0.87       408\n",
      "weighted avg       0.87      0.87      0.87       408\n",
      "\n",
      "Accuracy score (training): 0.994\n",
      "Accuracy score (test): 0.873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier=GradientBoostingClassifier()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "print('GradientBoostingClassifier:',accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(classifier.score(X_train, y_train)))\n",
    "print(\"Accuracy score (test): {0:.3f}\".format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09ca6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier: 0.6740196078431373\n",
      "[[71  6  3  0  5]\n",
      " [15 50  4  0  7]\n",
      " [ 8  6 43  0 20]\n",
      " [11  0 15 61  0]\n",
      " [11  4 18  0 50]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.84      0.71        85\n",
      "           1       0.76      0.66      0.70        76\n",
      "           2       0.52      0.56      0.54        77\n",
      "           3       1.00      0.70      0.82        87\n",
      "           4       0.61      0.60      0.61        83\n",
      "\n",
      "    accuracy                           0.67       408\n",
      "   macro avg       0.70      0.67      0.68       408\n",
      "weighted avg       0.70      0.67      0.68       408\n",
      "\n",
      "Accuracy score (training): 0.711\n",
      "Accuracy score (test): 0.674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier=AdaBoostClassifier()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred=classifier.predict(X_test)\n",
    "print('GradientBoostingClassifier:',accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(classifier.score(X_train, y_train)))\n",
    "print(\"Accuracy score (test): {0:.3f}\".format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3238d99",
   "metadata": {},
   "source": [
    "# Documnet classification with whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86548bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "#nltk.download('stopwords')\n",
    "import pickle\n",
    "#from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5e6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode(df, lst_cols, fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    if (lst_cols is not None\n",
    "        and len(lst_cols) > 0\n",
    "        and not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b64fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "my_data=load_files(r'C:\\ML Training Docs\\New folder\\data_final1')\n",
    "X,y = my_data.data, my_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900cda5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\ML Training Docs\\New folder\\data_final1\\NHS...</td>\n",
       "      <td>NHSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\ML Training Docs\\New folder\\data_final1\\3rd...</td>\n",
       "      <td>3rd Party Expert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\ML Training Docs\\New folder\\data_final1\\3rd...</td>\n",
       "      <td>3rd Party Expert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\ML Training Docs\\New folder\\data_final1\\GMC...</td>\n",
       "      <td>GMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\ML Training Docs\\New folder\\data_final1\\GMC...</td>\n",
       "      <td>GMC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           filenames            target\n",
       "0  C:\\ML Training Docs\\New folder\\data_final1\\NHS...             NHSLA\n",
       "0  C:\\ML Training Docs\\New folder\\data_final1\\3rd...  3rd Party Expert\n",
       "0  C:\\ML Training Docs\\New folder\\data_final1\\3rd...  3rd Party Expert\n",
       "0  C:\\ML Training Docs\\New folder\\data_final1\\GMC...               GMC\n",
       "0  C:\\ML Training Docs\\New folder\\data_final1\\GMC...               GMC"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.DataFrame({'filenames':[my_data.filenames],'target':[my_data.target]})\n",
    "df=explode(df,['filenames','target'],fill_value='',preserve_index=True)\n",
    "df['target']=df['target'].replace([0,1,2,3,4],['3rd Party Expert','GMC','Solicitor Contacts','MDU Invoices','NHSLA'])\n",
    "# df.to_csv(r'C:\\ML Training Docs\\New folder\\data_final\\data_final.csv',index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbadfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zz2839\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.set_proxy('http://Boopathi.Murugesan:Welcometothemdu2021@proxy.mdu.local:8080')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\\\n', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\\\t', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\\\', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r' +', ' ', document)\n",
    "    \n",
    "    #Remove special character\n",
    "    document = re.sub(r\"[^a-zA-Z0-9]+\", ' ', document)\n",
    "    \n",
    "#     #spaces between words and numbers substitute '' with ' '\n",
    "#     document = re.sub('\\W+','', document )\n",
    "    \n",
    "#     # Removing prefixed 'b'\n",
    "#     document = re.sub(r'^b\\s+', '', document)\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    # Remove double digit number\n",
    "    document=re.sub('(\\\\b[0-9][0-9] \\\\b|\\\\b [0-9][0-9]\\\\b)', ' ', document).strip()\n",
    "    #Remove single character\n",
    "    document=re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', ' ', document).strip()\n",
    "    # Remove single digit number\n",
    "    document=re.sub('(\\\\b[0-9] \\\\b|\\\\b [0-9]\\\\b)', ' ', document).strip()\n",
    "    \n",
    "    # Remove one char joins with 2 digit number\n",
    "    document= re.sub(r'[a-z]{1}[0-9]{2}', ' ', document)\n",
    "    # Remove two char joins with 1 digit number\n",
    "    document= re.sub(r'[a-z]{2}[0-9]{2}', ' ', document)\n",
    "    document= re.sub(r'[a-z]{1}[0-9]{2}[a-z]{1}', ' ', document)\n",
    "    document= re.sub(r'[a-z]{2}[0-9]{1}', ' ', document)\n",
    "#     document= re.sub(r'[a-z]{2}[0-9]{2}', ' ', document)\n",
    "#     document= re.sub(r'[0-9]{1}[a-z]{2}', ' ', document)\n",
    "#     document= re.sub(r'[0-9]{2}[a-z]{2}', ' ', document)\n",
    "#     document= re.sub(r'[a-z]{2}', ' ', document)\n",
    "    \n",
    "    #Remove page number of scanned documnet\n",
    "    document = re.sub(r'[0-9]{1,4}\\n',' ',document)\n",
    "    \n",
    "    #Remove scanned with camscanner text\n",
    "    document = re.sub('Scanned with CamScanner\\n',' ',document)\n",
    "    \n",
    "    #substitute multiple space with single space\n",
    "#     document = re.sub(r'\\s{1,30}',' ',document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(e for e in document if e.isalnum())\n",
    "    document = ' '.join([w for w in document.split() if len(w)>1])\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document= re.sub(r'[a-z]{2}[0-9]{1}', ' ', document)\n",
    "# document= re.sub(r'[a-z]{2}[0-9]{2}', ' ', document)\n",
    "# document= re.sub(r'[0-9]{1}[a-z]{2}', ' ', document)\n",
    "# document= re.sub(r'[[0-9]{2}[a-z]{2}', ' ', document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f36db39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear mr jafari re performance advisory group on and dental record review nh england hold the national dental performer list and is responsible for investigating concern raised about performer this duty is discharged through the performer list decision panel pldp and performance advisory group pag the pag screen all concern raised about performer in accordance with the nh england performer list regulation 2013 you will be aware that dental record review of patient record from your dental care wa recently undertaken by an nh england dental clinical adviser the review wa tabled at the pag on march 2016 whilst the review in it entirety will be shared with you in due course have added the adviser conclusion and recommendation below conclusion record keeping the overall record keeping is satisfactory but with some reservation there is use of pre populated template and with these it is difficult to confirm to whether item noted have actually been carried out quality of care there is concern that all the periodontal treatment including scale and polish are left for the hygienist it is not clear from the note if treatment by the hygienist is ever offered under the nh it appears that even the periodontal assessment treatment and any monitoring is left to the hygienist thus the british society of periodontology guideline of parameter of care are not adhered to comprehensive periodontal assessment wa not provided for patient ta although indicated bpe score of in all sextant wa recorded the adviser considers this pattern of scoring to be unusual particularly where teeth were noted mobile with an upper incisor having grade mobility private confidential for the attention of addressee only mr jafari your dental care sackville road bexhill on sea east sussex tn39 3jd and via email arashjafari btinternet com nh england south south east medical directorate york house massetts road horley surrey rh6 7de telephone 0113 825 4733 email amanda sale nh net march 2016 dear mr jafari re performance advisory group on and dental record review nh england hold the national dental performer list and is responsible for investigating concern raised about performer this duty is discharged through the performer list decision panel pldp and performance advisory group pag the pag screen all concern raised about performer in accordance with the nh england performer list regulation 2013 you will be aware that dental record review of patient record from your dental care wa recently undertaken by an nh england dental clinical adviser the review wa tabled at the pag on march 2016 whilst the review in it entirety will be shared with you in due course have added the adviser conclusion and recommendation below conclusion record keeping the overall record keeping is satisfactory but with some reservation there is use of pre populated template and with these it is difficult to confirm to whether item noted have actually been carried out quality of care there is concern that all the periodontal treatment including scale and polish are left for the hygienist it is not clear from the note if treatment by the hygienist is ever offered under the nh it appears that even the periodontal assessment treatment and any monitoring is left to the hygienist thus the british society of periodontology guideline of parameter of care are not adhered to comprehensive periodontal assessment wa not provided for patient ta although indicated bpe score of in all sextant wa recorded the adviser considers this pattern of scoring to be unusual particularly where teeth were noted mobile with an upper incisor having grade mobility private confidential for the attention of addressee only mr jafari your dental care sackville road bexhill on sea east sussex tn39 3jd and via email arashjafari btinternet com nh england south south east medical directorate york house massetts road horley surrey rh6 7de telephone 0113 825 4733 email amanda sale nh net march 2016 there is one case whereby an earlier diagnostic ray could possibly have prevented patient suffering pain and resultant further visit to have filling carried out on different tooth involving an additional claim there is overall tendency to carry out minimal treatment metal crown wa provided on an upper pre molar tooth although technically possible under the nh tooth coloured crown would have been preferable option probity there is one inappropriate claim where an examination and filling provided on separate visit should have been just one claim and not two separate claim recommendation that you attend meeting with nh england to discus xef the periodontal aspect of the patient and whether they are offered periodontal treatment under the nh xef use of pre populated template with specific reference to any discussion with patient the pre populated template are discouraged by the gdc when not used correctly further record card review in six month time of at least patient record involving band and claim preferably involving multiple claim the pag panel agreed that you should be invited in to attend for meeting with dental clinical adviser and myself the record of the meeting would be tabled at the next pag on april 2016 please confirm your availability for 10am meeting on tuesday april 2016 at our horley office address per letterhead please confirm your availability by email at amanda sale nh net by no later than tuesday march 2016 you may wish to bring colleague friend or local representative committee or defence union representative with you or seek their advice prior to this meeting if you are bringing companion with you please let me know the name and designation of that person am attaching support leaflet yours sincerely mandy sale dental case manager professional performance attachment support leaflet to follow dental record review in it entirety ahead of our face to face meeting']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2025dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = documents\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "clean_document = [' '.join([b for b in a.split() if b not in stop_words]) for a in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069e3125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear mr jafari performance advisory group dental record review nh england hold national dental performer list responsible investigating concern raised performer duty discharged performer list decision panel pldp performance advisory group pag pag screen concern raised performer accordance nh england performer list regulation 2013 aware dental record review patient record dental care wa recently undertaken nh england dental clinical adviser review wa tabled pag march 2016 whilst review entirety shared due course added adviser conclusion recommendation conclusion record keeping overall record keeping satisfactory reservation use pre populated template difficult confirm whether item noted actually carried quality care concern periodontal treatment including scale polish left hygienist clear note treatment hygienist ever offered nh appears even periodontal assessment treatment monitoring left hygienist thus british society periodontology guideline parameter care adhered comprehensive periodontal assessment wa provided patient ta although indicated bpe score sextant wa recorded adviser considers pattern scoring unusual particularly teeth noted mobile upper incisor grade mobility private confidential attention addressee mr jafari dental care sackville road bexhill sea east sussex tn39 3jd via email arashjafari btinternet com nh england south south east medical directorate york house massetts road horley surrey rh6 7de telephone 0113 825 4733 email amanda sale nh net march 2016 dear mr jafari performance advisory group dental record review nh england hold national dental performer list responsible investigating concern raised performer duty discharged performer list decision panel pldp performance advisory group pag pag screen concern raised performer accordance nh england performer list regulation 2013 aware dental record review patient record dental care wa recently undertaken nh england dental clinical adviser review wa tabled pag march 2016 whilst review entirety shared due course added adviser conclusion recommendation conclusion record keeping overall record keeping satisfactory reservation use pre populated template difficult confirm whether item noted actually carried quality care concern periodontal treatment including scale polish left hygienist clear note treatment hygienist ever offered nh appears even periodontal assessment treatment monitoring left hygienist thus british society periodontology guideline parameter care adhered comprehensive periodontal assessment wa provided patient ta although indicated bpe score sextant wa recorded adviser considers pattern scoring unusual particularly teeth noted mobile upper incisor grade mobility private confidential attention addressee mr jafari dental care sackville road bexhill sea east sussex tn39 3jd via email arashjafari btinternet com nh england south south east medical directorate york house massetts road horley surrey rh6 7de telephone 0113 825 4733 email amanda sale nh net march 2016 one case whereby earlier diagnostic ray could possibly prevented patient suffering pain resultant visit filling carried different tooth involving additional claim overall tendency carry minimal treatment metal crown wa provided upper pre molar tooth although technically possible nh tooth coloured crown would preferable option probity one inappropriate claim examination filling provided separate visit one claim two separate claim recommendation attend meeting nh england discus xef periodontal aspect patient whether offered periodontal treatment nh xef use pre populated template specific reference discussion patient pre populated template discouraged gdc used correctly record card review six month time least patient record involving band claim preferably involving multiple claim pag panel agreed invited attend meeting dental clinical adviser record meeting would tabled next pag april 2016 please confirm availability 10am meeting tuesday april 2016 horley office address per letterhead please confirm availability email amanda sale nh net later tuesday march 2016 may wish bring colleague friend local representative committee defence union representative seek advice prior meeting bringing companion please let know name designation person attaching support leaflet sincerely mandy sale dental case manager professional performance attachment support leaflet follow dental record review entirety ahead face face meeting']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_document[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1fe2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7)\n",
    "X = vectorizer.fit_transform(clean_document).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5245fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97c234ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96a96cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_forest_training time: 1.803 s\n",
      "RandomForest: 0.8501683501683501\n",
      "[[108   5   5   0   9]\n",
      " [ 13 158   2   1  10]\n",
      " [  5   9  61   2   7]\n",
      " [  1   0   7 119   0]\n",
      " [  9   3   1   0  59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       127\n",
      "           1       0.90      0.86      0.88       184\n",
      "           2       0.80      0.73      0.76        84\n",
      "           3       0.98      0.94      0.96       127\n",
      "           4       0.69      0.82      0.75        72\n",
      "\n",
      "    accuracy                           0.85       594\n",
      "   macro avg       0.83      0.84      0.83       594\n",
      "weighted avg       0.86      0.85      0.85       594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''training the data in RF model'''\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "classifier=RandomForestClassifier()\n",
    "start = time.time()\n",
    "classifier.fit(X_train,y_train)\n",
    "# the time would be round to 3 decimal in seconds\n",
    "end = time.time()\n",
    "print (\"Random_forest_training time:\", round(end-start, 3), \"s\")\n",
    "# t1=time.time()\n",
    "y_pred=classifier.predict(X_test)\n",
    "# print (\"predict time:\", round(end-t1, 3), \"s\")\n",
    "print('RandomForest:',accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3e8f7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient_Boost_training time: 152.803 s\n",
      "GradientBoostingClassifier: 0.8585858585858586\n",
      "[[106   3   5   0  13]\n",
      " [  9 163   2   0  10]\n",
      " [  4   8  64   3   5]\n",
      " [  0   0   6 121   0]\n",
      " [  7   4   5   0  56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.84       127\n",
      "           1       0.92      0.89      0.90       184\n",
      "           2       0.78      0.76      0.77        84\n",
      "           3       0.98      0.95      0.96       127\n",
      "           4       0.67      0.78      0.72        72\n",
      "\n",
      "    accuracy                           0.86       594\n",
      "   macro avg       0.84      0.84      0.84       594\n",
      "weighted avg       0.86      0.86      0.86       594\n",
      "\n",
      "Accuracy score (training): 0.980\n",
      "Accuracy score (test): 0.859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier=GradientBoostingClassifier()\n",
    "start = time.time()\n",
    "classifier.fit(X_train,y_train)\n",
    "end = time.time()\n",
    "print (\"Gradient_Boost_training time:\", round(end-start, 3), \"s\")\n",
    "y_pred=classifier.predict(X_test)\n",
    "print('GradientBoostingClassifier:',accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(classifier.score(X_train, y_train)))\n",
    "print(\"Accuracy score (test): {0:.3f}\".format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d66e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
